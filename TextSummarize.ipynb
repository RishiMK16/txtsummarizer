{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Summarizer\n",
        "Simple text summarizer using NLTK for natural language processing"
      ],
      "metadata": {
        "id": "DIUgLRtPdkyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # This is what we need for sentence tokenization\n",
        "nltk.download('stopwords')  # This is for stop words\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Verify the downloads\n",
        "import nltk.data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "    print(\"All required NLTK data is downloaded and available!\")\n",
        "except LookupError as e:\n",
        "    print(\"Error: Some required NLTK data is missing:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPvighdjdmxg",
        "outputId": "a7085d9c-348e-4154-b91e-5debcffe4087"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All required NLTK data is downloaded and available!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_frequency_table(text):\n",
        "    \"\"\"Create a frequency table for words in the text.\"\"\"\n",
        "    words = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stopwords and punctuation\n",
        "    stop_words = set(stopwords.words('english') + list(punctuation))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Calculate frequency of each word\n",
        "    freq_table = defaultdict(int)\n",
        "    for word in words:\n",
        "        freq_table[word] += 1\n",
        "\n",
        "    return freq_table"
      ],
      "metadata": {
        "id": "MbG69zNpdr50"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score_sentences(sentences, freq_table):\n",
        "    \"\"\"Score sentences based on word frequencies.\"\"\"\n",
        "    sentence_scores = defaultdict(int)\n",
        "\n",
        "    for sentence in sentences:\n",
        "        word_count = len(word_tokenize(sentence))\n",
        "        if word_count <= 30:  # Ignore very long sentences\n",
        "            for word in word_tokenize(sentence.lower()):\n",
        "                if word in freq_table:\n",
        "                    sentence_scores[sentence] += freq_table[word]\n",
        "\n",
        "            # Normalize score by sentence length\n",
        "            sentence_scores[sentence] = sentence_scores[sentence] / word_count\n",
        "\n",
        "    return sentence_scores"
      ],
      "metadata": {
        "id": "f8LXgR0rdubs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text(text, num_sentences=3):\n",
        "    \"\"\"Generate a summary of the text.\"\"\"\n",
        "    # Tokenize text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Create frequency table of words\n",
        "    freq_table = create_frequency_table(text)\n",
        "\n",
        "    # Score sentences\n",
        "    sentence_scores = score_sentences(sentences, freq_table)\n",
        "\n",
        "    # Get top scoring sentences\n",
        "    summary_sentences = sorted(sentence_scores.items(),\n",
        "                             key=lambda x: x[1],\n",
        "                             reverse=True)[:num_sentences]\n",
        "\n",
        "    # Sort sentences by their original order\n",
        "    summary_sentences.sort(key=lambda x: sentences.index(x[0]))\n",
        "\n",
        "    # Join sentences to create summary\n",
        "    summary = ' '.join(sentence[0] for sentence in summary_sentences)\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "LvFjlj1xdxRh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the summarizer with sample text"
      ],
      "metadata": {
        "id": "dw7-OwH-d0GV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text\n",
        "text = \"\"\"\n",
        "Machine learning is a field of artificial intelligence that focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy. Machine learning is an important component of the growing field of data science. Through the use of statistical methods, algorithms are trained to make classifications or predictions, uncovering key insights within data mining projects. These insights subsequently drive decision making within applications and businesses, ideally impacting key growth metrics. As big data continues to expand and grow, the market demand for data scientists will increase. They will be required to help identify the most relevant business questions and subsequently the data to answer them.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print(\"\\nSummary (2 sentences):\")\n",
        "summary = summarize_text(text, num_sentences=2)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chfDHTJSd1lz",
        "outputId": "fe03cc14-faf5-4f02-eade-4b22e8c1fbae"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "\n",
            "Machine learning is a field of artificial intelligence that focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy. Machine learning is an important component of the growing field of data science. Through the use of statistical methods, algorithms are trained to make classifications or predictions, uncovering key insights within data mining projects. These insights subsequently drive decision making within applications and businesses, ideally impacting key growth metrics. As big data continues to expand and grow, the market demand for data scientists will increase. They will be required to help identify the most relevant business questions and subsequently the data to answer them.\n",
            "\n",
            "\n",
            "Summary (2 sentences):\n",
            "Machine learning is an important component of the growing field of data science. As big data continues to expand and grow, the market demand for data scientists will increase.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "your_text = \"\"\"\n",
        "Elasticsearch is an open source distributed, RESTful search and analytics engine, scalable data store, and vector database capable of addressing a growing number of use cases. As the heart of the Elastic Stack, it centrally stores your data for lightning-fast search, fine‑tuned relevancy, and powerful analytics that scale with ease.\n",
        "\"\"\"\n",
        "\n",
        "summary = summarize_text(your_text, num_sentences=3)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtHLdWLQd6VN",
        "outputId": "09fad304-3f16-41ed-a17c-fd76f9cb5de6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Elasticsearch is an open source distributed, RESTful search and analytics engine, scalable data store, and vector database capable of addressing a growing number of use cases. As the heart of the Elastic Stack, it centrally stores your data for lightning-fast search, fine‑tuned relevancy, and powerful analytics that scale with ease.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "T9LRPCfIdivK"
      }
    }
  ]
}